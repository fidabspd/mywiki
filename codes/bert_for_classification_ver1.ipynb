{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225e7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import transformers\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a28b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "\n",
    "train_origin = pd.read_csv(data_path+'train_data.csv')\n",
    "test_origin = pd.read_csv(data_path+'test_data.csv')\n",
    "topic_dict_origin = pd.read_csv(data_path+'topic_dict.csv')\n",
    "sample_submission_origin = pd.read_csv(data_path+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af7c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_origin.copy()\n",
    "test = test_origin.copy()\n",
    "topic_dict = topic_dict_origin.copy()\n",
    "sample_submission = sample_submission_origin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4360e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(\n",
    "    'kykim/bert-kor-base',\n",
    "    cache_dir = '../tokenizer/kykim/bert-kor-base',\n",
    "    do_lower_case = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d990158",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {}\n",
    "data_config['max_length'] = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df6de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    text = re.sub('[^a-z가-힣]', ' ', text.lower())\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e352a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['title'].apply(cleaning_text)\n",
    "X_train = X_train.values.tolist()\n",
    "\n",
    "X_train_encoded = tokenizer(\n",
    "    X_train,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    max_length = data_config['max_length'],\n",
    "    return_tensors = 'tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8825312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['topic_idx'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d6dffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = transformers.TFBertForSequenceClassification.from_pretrained(\n",
    "    'kykim/bert-kor-base', \n",
    "    cache_dir = '../model/kykim/bert-kor-base/cache/',\n",
    "    output_hidden_states=False,\n",
    "    output_attentions=False,\n",
    "    use_cache = False,\n",
    "    num_labels = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08f370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(learning_rate=3e-5, print_summary=False):\n",
    "\n",
    "    input_ids = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='input_ids')\n",
    "    input_masks = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='attention_masks')\n",
    "    bert_outputs = bert_model([input_ids, input_masks])['logits']\n",
    "    dropout_0 = Dropout(0.1, name='dropout_0')(bert_outputs)\n",
    "    dense_0 = Dense(32, activation='relu', name='dense_0')(dropout_0)\n",
    "    dropout_1 = Dropout(0.1, name='dropout_1')(dense_0)\n",
    "    outputs = Dense(7, activation='softmax', name='outputs')(dropout_1)\n",
    "\n",
    "    model = Model(\n",
    "        inputs = [input_ids, input_masks],\n",
    "        outputs = outputs,\n",
    "        name = 'Bert_Classification'\n",
    "    )\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        optimizer = optimizer, \n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    if print_summary:\n",
    "        model.summary(line_length=150)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea7c744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f3c59f2bba8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f3c57dd9110> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f3c59f2bba8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f3c57dd9110> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Model: \"Bert_Classification\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 44)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 44)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_for_sequence_classifica TFSequenceClassifier 118346560   input_ids[0][0]                  \n",
      "                                                                 attention_masks[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_0 (Dropout)             (None, 64)           0           tf_bert_for_sequence_classificati\n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 32)           2080        dropout_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32)           0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 7)            231         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 118,348,871\n",
      "Trainable params: 2,311\n",
      "Non-trainable params: 118,346,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = set_model(learning_rate=9e-5)\n",
    "model.layers[2].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "875abe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "input_ids = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='input_ids')\n",
    "input_masks = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='attention_masks')\n",
    "bert_outputs = bert_model([input_ids, input_masks])['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17583885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "714/714 [==============================] - 226s 305ms/step - loss: 0.5643 - accuracy: 0.8244\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x = [X_train_encoded['input_ids'], X_train_encoded['attention_mask']], \n",
    "    y = y_train,\n",
    "    epochs = 1,\n",
    "    batch_size = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "909911c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    optimizer = optimizer, \n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6159145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1142/1142 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.8964WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1142/1142 [==============================] - 225s 189ms/step - loss: 0.3271 - accuracy: 0.8964 - val_loss: 0.2940 - val_accuracy: 0.9024\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.90242, saving model to ../model/bert_kor_base_for_classification_fine_tuning_0.h5\n",
      "Epoch 2/15\n",
      "1142/1142 [==============================] - 212s 186ms/step - loss: 0.2421 - accuracy: 0.9234 - val_loss: 0.3148 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.90242\n",
      "Epoch 3/15\n",
      "1142/1142 [==============================] - 212s 186ms/step - loss: 0.1756 - accuracy: 0.9438 - val_loss: 0.3652 - val_accuracy: 0.8907\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.90242\n",
      "Epoch 4/15\n",
      "1142/1142 [==============================] - 212s 185ms/step - loss: 0.1264 - accuracy: 0.9601 - val_loss: 0.3934 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.90242\n",
      "Epoch 5/15\n",
      "1142/1142 [==============================] - 214s 187ms/step - loss: 0.0945 - accuracy: 0.9708 - val_loss: 0.4787 - val_accuracy: 0.8852\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.90242\n",
      "Epoch 6/15\n",
      "1142/1142 [==============================] - 212s 185ms/step - loss: 0.0735 - accuracy: 0.9775 - val_loss: 0.4722 - val_accuracy: 0.8894\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.90242\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 5\n",
    ")\n",
    "model_check_point = ModelCheckpoint(\n",
    "    filepath = '../model/bert_kor_base_for_classification_fine_tuning_0.h5',\n",
    "    monitor = 'val_accuracy',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True\n",
    ")\n",
    "def scheduler(epoch, lr):\n",
    "    return lr * 0.95\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "history = model.fit(\n",
    "    x = [X_train_encoded['input_ids'], X_train_encoded['attention_mask']], \n",
    "    y = y_train,\n",
    "    epochs = 15,\n",
    "    batch_size = 32,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [early_stop, model_check_point, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abab8c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "best_model = set_model()\n",
    "best_model.load_weights('../model/bert_kor_base_for_classification_fine_tuning_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b66f1d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "train accuracy: 0.9304\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = best_model.predict([X_train_encoded['input_ids'], X_train_encoded['attention_mask']])\n",
    "y_train_pred = np.argmax(y_train_pred, axis=1)\n",
    "\n",
    "print(f'train accuracy: {accuracy_score(y_train, y_train_pred):.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2501c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test['title'].apply(cleaning_text)\n",
    "X_test = X_test.values.tolist()\n",
    "\n",
    "X_test_encoded = tokenizer(\n",
    "    X_test,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    max_length = data_config['max_length'],\n",
    "    return_tensors = 'tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b26f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict([X_test_encoded['input_ids'], X_test_encoded['attention_mask']])\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "sample_submission['topic_idx'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b4db88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission.to_csv('../submit/bert_kor_base_for_classification_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ff66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
