{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225e7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import transformers\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a28b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "\n",
    "train_origin = pd.read_csv(data_path+'train_data.csv')\n",
    "test_origin = pd.read_csv(data_path+'test_data.csv')\n",
    "topic_dict_origin = pd.read_csv(data_path+'topic_dict.csv')\n",
    "sample_submission_origin = pd.read_csv(data_path+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af7c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_origin.copy()\n",
    "test = test_origin.copy()\n",
    "topic_dict = topic_dict_origin.copy()\n",
    "sample_submission = sample_submission_origin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4360e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    'bert-base-multilingual-cased', \n",
    "    cache_dir = '../tokenizer/bert-base-multilingual-cased',\n",
    "    do_lower_case = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d990158",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {}\n",
    "data_config['max_length'] = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df6de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    text = re.sub('[^a-z가-힣]', ' ', text.lower())\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e352a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['title'].apply(cleaning_text)\n",
    "X_train = X_train.values.tolist()\n",
    "\n",
    "X_train_encoded = tokenizer(\n",
    "    X_train,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    max_length = data_config['max_length'],\n",
    "    return_tensors = 'tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8825312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['topic_idx'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d6dffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_config = transformers.BertConfig.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    cache_dir = '../model/bert-base-multilingual-cased/cache/',\n",
    "    output_hidden_states=False,\n",
    "    output_attentions=False,\n",
    "    use_cache = False\n",
    ")\n",
    "\n",
    "bert_model = transformers.TFBertModel.from_pretrained(\n",
    "    'bert-base-multilingual-cased', \n",
    "    config = bert_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08f370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(print_summary=False):\n",
    "\n",
    "    input_ids = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='input_ids')\n",
    "    input_masks = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='attention_masks')\n",
    "    bert_outputs = bert_model([input_ids, input_masks])[1]\n",
    "    dropout_0 = Dropout(0.1)(bert_outputs)\n",
    "    dense_0 = Dense(64, activation='relu', name='dense_0')(dropout_0)\n",
    "    dropout_1 = Dropout(0.1)(dense_0)\n",
    "    dense_1 = Dense(32, activation='relu', name='dense_1')(dropout_1)\n",
    "    dropout_2 = Dropout(0.1)(dense_1)\n",
    "    outputs = Dense(7, activation='softmax', name='outputs')(dropout_2)\n",
    "    \n",
    "#     input_ids = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='input_ids')\n",
    "#     input_masks = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='attention_masks')\n",
    "#     bert_outputs = bert_model([input_ids, input_masks])[1]\n",
    "#     dropout_0 = Dropout(0.1)(bert_outputs)\n",
    "#     outputs = Dense(7, activation='softmax', name='outputs')(dropout_0)\n",
    "\n",
    "    model = Model(\n",
    "        inputs = [input_ids, input_masks],\n",
    "        outputs = outputs,\n",
    "        name = 'Bert_Classification'\n",
    "    )\n",
    "\n",
    "    # bert_model.trainable = False  # bert는 train 해제. pre-trained 그대로 사용\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        optimizer = optimizer, \n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    if print_summary:\n",
    "        model.summary(line_length=150)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea7c744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8bb21c6ba8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f8bb0074110> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8bb21c6ba8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f8bb0074110> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Model: \"Bert_Classification\"\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "input_ids (InputLayer)                           [(None, 44)]                     0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)                     [(None, 44)]                     0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)                      TFBaseModelOutputWithPooling(las 177853440         input_ids[0][0]                                   \n",
      "                                                                                                    attention_masks[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)                             (None, 768)                      0                 tf_bert_model[0][1]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense_0 (Dense)                                  (None, 64)                       49216             dropout_37[0][0]                                  \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)                             (None, 64)                       0                 dense_0[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense_1 (Dense)                                  (None, 32)                       2080              dropout_38[0][0]                                  \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)                             (None, 32)                       0                 dense_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "outputs (Dense)                                  (None, 7)                        231               dropout_39[0][0]                                  \n",
      "======================================================================================================================================================\n",
      "Total params: 177,904,967\n",
      "Trainable params: 177,904,967\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = set_model(print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6159145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1142/1142 [==============================] - ETA: 0s - loss: 0.7830 - accuracy: 0.7540WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1142/1142 [==============================] - 242s 205ms/step - loss: 0.7830 - accuracy: 0.7540 - val_loss: 0.8449 - val_accuracy: 0.7086\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.70858, saving model to ../model/bert_fine_tuning_0.h5\n",
      "Epoch 2/10\n",
      "1142/1142 [==============================] - 234s 205ms/step - loss: 0.5187 - accuracy: 0.8403 - val_loss: 0.7377 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.70858 to 0.75271, saving model to ../model/bert_fine_tuning_0.h5\n",
      "Epoch 3/10\n",
      "1142/1142 [==============================] - 233s 204ms/step - loss: 0.4119 - accuracy: 0.8710 - val_loss: 0.6385 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.75271 to 0.78568, saving model to ../model/bert_fine_tuning_0.h5\n",
      "Epoch 4/10\n",
      "1142/1142 [==============================] - 232s 204ms/step - loss: 0.3426 - accuracy: 0.8920 - val_loss: 0.7338 - val_accuracy: 0.7642\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.78568\n",
      "Epoch 5/10\n",
      "1142/1142 [==============================] - 233s 204ms/step - loss: 0.2905 - accuracy: 0.9076 - val_loss: 0.7388 - val_accuracy: 0.7845\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.78568\n",
      "Epoch 6/10\n",
      "1142/1142 [==============================] - 231s 202ms/step - loss: 0.2325 - accuracy: 0.9270 - val_loss: 0.8098 - val_accuracy: 0.7828\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.78568\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3\n",
    ")\n",
    "model_check_point = ModelCheckpoint(\n",
    "    filepath = '../model/bert_fine_tuning_0.h5',\n",
    "    monitor = 'val_accuracy',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True\n",
    ")\n",
    "def scheduler(epoch, lr):\n",
    "    return lr * 0.95\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "history = model.fit(\n",
    "    x = [X_train_encoded['input_ids'], X_train_encoded['attention_mask']], \n",
    "    y = y_train,\n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [early_stop, model_check_point, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abab8c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "best_model = set_model()\n",
    "best_model.load_weights('../model/bert_fine_tuning_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b66f1d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "train accuracy: 0.8888\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = best_model.predict([X_train_encoded['input_ids'], X_train_encoded['attention_mask']])\n",
    "y_train_pred = np.argmax(y_train_pred, axis=1)\n",
    "\n",
    "print(f'train accuracy: {accuracy_score(y_train, y_train_pred):.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2501c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test['title']\n",
    "X_test = X_test.values.tolist()\n",
    "\n",
    "X_test_encoded = tokenizer(\n",
    "    X_test,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    max_length = data_config['max_length'],\n",
    "    return_tensors = 'tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b26f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict([X_test_encoded['input_ids'], X_test_encoded['attention_mask']])\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "sample_submission['topic_idx'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b4db88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission.to_csv('../submit/bert_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfd027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
