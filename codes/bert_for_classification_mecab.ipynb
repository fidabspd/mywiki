{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225e7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import transformers\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a28b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "\n",
    "train_origin = pd.read_csv(data_path+'train_preprocessed.csv')\n",
    "test_origin = pd.read_csv(data_path+'test_preprocessed.csv')\n",
    "topic_dict_origin = pd.read_csv(data_path+'topic_dict.csv')\n",
    "sample_submission_origin = pd.read_csv(data_path+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af7c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_origin.copy()\n",
    "test = test_origin.copy()\n",
    "topic_dict = topic_dict_origin.copy()\n",
    "sample_submission = sample_submission_origin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b11180",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_sentence = lambda stem_noun: '[CLS] '+' '.join(eval(stem_noun))+' [SEP]'\n",
    "train['stem_noun'] = train['stem_noun'].apply(list_to_sentence)\n",
    "test['stem_noun'] = test['stem_noun'].apply(list_to_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794bc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_seq(train_clean, test_clean, max_len=None, padding='post'):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_clean)\n",
    "    train_seq = tokenizer.texts_to_sequences(train_clean)\n",
    "    test_seq = tokenizer.texts_to_sequences(test_clean)\n",
    "    vocabulary = tokenizer.word_index\n",
    "    X_train = pad_sequences(train_seq, maxlen=max_len, padding=padding)\n",
    "    X_test = pad_sequences(test_seq, maxlen=max_len, padding=padding)\n",
    "    return X_train, X_test, vocabulary, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606b34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {}\n",
    "data_config['max_length'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a424ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = {}\n",
    "X_test_encoded = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d40436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45654, 50), (9131, 50))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded['input_ids'], X_test_encoded['input_ids'], vocabulary, vectorizer = txt_to_seq(train['stem_noun'], test['stem_noun'], max_len=data_config['max_length'], padding='post')\n",
    "X_train_encoded['input_ids'].shape, X_test_encoded['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44bbe9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded['attention_mask'] = np.zeros(X_train_encoded['input_ids'].shape)\n",
    "X_test_encoded['attention_mask'] = np.zeros(X_test_encoded['input_ids'].shape)\n",
    "\n",
    "for row in range(X_train_encoded['attention_mask'].shape[0]):\n",
    "    X_train_encoded['attention_mask'][row] = [1 if i else 0 for i in X_train_encoded['input_ids'][row]]\n",
    "for row in range(X_test_encoded['attention_mask'].shape[0]):\n",
    "    X_test_encoded['attention_mask'][row] = [1 if i else 0 for i in X_test_encoded['input_ids'][row]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8825312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['topic_idx'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6dffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = transformers.TFBertForSequenceClassification.from_pretrained(\n",
    "    'kykim/bert-kor-base', \n",
    "    cache_dir = '../model/kykim/bert-kor-base/cache/',\n",
    "    output_hidden_states=False,\n",
    "    output_attentions=False,\n",
    "    use_cache = False,\n",
    "    num_labels = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e08f370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(learning_rate=3e-5, print_summary=False):\n",
    "\n",
    "    input_ids = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='input_ids')\n",
    "    input_masks = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='attention_masks')\n",
    "    bert_outputs = bert_model([input_ids, input_masks])['logits']\n",
    "    dropout_0 = Dropout(0.1, name='dropout_0')(bert_outputs)\n",
    "    dense_0 = Dense(32, activation='relu', name='dense_0')(dropout_0)\n",
    "    dropout_1 = Dropout(0.1, name='dropout_1')(dense_0)\n",
    "    outputs = Dense(7, activation='softmax', name='outputs')(dropout_1)\n",
    "\n",
    "    model = Model(\n",
    "        inputs = [input_ids, input_masks],\n",
    "        outputs = outputs,\n",
    "        name = 'Bert_Classification'\n",
    "    )\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        optimizer = optimizer, \n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    if print_summary:\n",
    "        model.summary(line_length=150)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea7c744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7ffac1093ba8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7ffabef40110> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7ffac1093ba8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7ffabef40110> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Model: \"Bert_Classification\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_for_sequence_classifica TFSequenceClassifier 118346560   input_ids[0][0]                  \n",
      "                                                                 attention_masks[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_0 (Dropout)             (None, 64)           0           tf_bert_for_sequence_classificati\n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 32)           2080        dropout_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32)           0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 7)            231         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 118,348,871\n",
      "Trainable params: 2,311\n",
      "Non-trainable params: 118,346,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = set_model(learning_rate=9e-5)\n",
    "model.layers[2].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "875abe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "input_ids = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='input_ids')\n",
    "input_masks = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='attention_masks')\n",
    "bert_outputs = bert_model([input_ids, input_masks])['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17583885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "16/16 [==============================] - 14s 344ms/step - loss: 1.5807 - accuracy: 0.3180\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x = [X_train_encoded['input_ids'][:1000], X_train_encoded['attention_mask'][:1000]], \n",
    "    y = y_train[:1000],\n",
    "    epochs = 1,\n",
    "    batch_size = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "909911c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    optimizer = optimizer, \n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6159145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1142/1142 [==============================] - ETA: 0s - loss: 1.7158 - accuracy: 0.2644WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1142/1142 [==============================] - 241s 204ms/step - loss: 1.7158 - accuracy: 0.2644 - val_loss: 1.8318 - val_accuracy: 0.2608\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.26076, saving model to ../model/bert_kor_base_for_classification_fine_tuning_mecab_0.h5\n",
      "Epoch 2/15\n",
      "1142/1142 [==============================] - 230s 201ms/step - loss: 1.2411 - accuracy: 0.4930 - val_loss: 1.3592 - val_accuracy: 0.4630\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.26076 to 0.46304, saving model to ../model/bert_kor_base_for_classification_fine_tuning_mecab_0.h5\n",
      "Epoch 3/15\n",
      "1142/1142 [==============================] - 229s 201ms/step - loss: 0.8632 - accuracy: 0.7006 - val_loss: 1.2900 - val_accuracy: 0.5504\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.46304 to 0.55043, saving model to ../model/bert_kor_base_for_classification_fine_tuning_mecab_0.h5\n",
      "Epoch 4/15\n",
      "1142/1142 [==============================] - 229s 201ms/step - loss: 0.6782 - accuracy: 0.7844 - val_loss: 1.3057 - val_accuracy: 0.5724\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.55043 to 0.57245, saving model to ../model/bert_kor_base_for_classification_fine_tuning_mecab_0.h5\n",
      "Epoch 5/15\n",
      "1142/1142 [==============================] - 232s 203ms/step - loss: 0.5742 - accuracy: 0.8271 - val_loss: 1.0086 - val_accuracy: 0.6760\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.57245 to 0.67605, saving model to ../model/bert_kor_base_for_classification_fine_tuning_mecab_0.h5\n",
      "Epoch 6/15\n",
      "1142/1142 [==============================] - 230s 201ms/step - loss: 0.4941 - accuracy: 0.8550 - val_loss: 1.0828 - val_accuracy: 0.6552\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.67605\n",
      "Epoch 7/15\n",
      "1142/1142 [==============================] - 229s 201ms/step - loss: 0.4411 - accuracy: 0.8717 - val_loss: 1.0006 - val_accuracy: 0.6999\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.67605 to 0.69992, saving model to ../model/bert_kor_base_for_classification_fine_tuning_mecab_0.h5\n",
      "Epoch 8/15\n",
      "1142/1142 [==============================] - 229s 200ms/step - loss: 0.3821 - accuracy: 0.8909 - val_loss: 1.1205 - val_accuracy: 0.6673\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.69992\n",
      "Epoch 9/15\n",
      "1142/1142 [==============================] - 229s 200ms/step - loss: 0.3392 - accuracy: 0.9046 - val_loss: 1.0261 - val_accuracy: 0.6937\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.69992\n",
      "Epoch 10/15\n",
      "1142/1142 [==============================] - 231s 202ms/step - loss: 0.2998 - accuracy: 0.9173 - val_loss: 1.0604 - val_accuracy: 0.7023\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.69992 to 0.70233, saving model to ../model/bert_kor_base_for_classification_fine_tuning_mecab_0.h5\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3\n",
    ")\n",
    "model_check_point = ModelCheckpoint(\n",
    "    filepath = '../model/bert_kor_base_for_classification_fine_tuning_mecab_0.h5',\n",
    "    monitor = 'val_accuracy',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True\n",
    ")\n",
    "def scheduler(epoch, lr):\n",
    "    return lr * 0.95\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "history = model.fit(\n",
    "    x = [X_train_encoded['input_ids'], X_train_encoded['attention_mask']], \n",
    "    y = y_train,\n",
    "    epochs = 15,\n",
    "    batch_size = 32,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [early_stop, model_check_point, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abab8c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "best_model = set_model()\n",
    "best_model.load_weights('../model/bert_kor_base_for_classification_fine_tuning_mecab_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b66f1d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "train accuracy: 0.9005\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = best_model.predict([X_train_encoded['input_ids'], X_train_encoded['attention_mask']])\n",
    "y_train_pred = np.argmax(y_train_pred, axis=1)\n",
    "\n",
    "print(f'train accuracy: {accuracy_score(y_train, y_train_pred):.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2501c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict([X_test_encoded['input_ids'], X_test_encoded['attention_mask']])\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b4db88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['topic_idx'] = y_test_pred\n",
    "sample_submission.to_csv('../submit/bert_kor_base_for_classification_mecab_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ff66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
