{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225e7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import transformers\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a28b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "\n",
    "train_origin = pd.read_csv(data_path+'train_data.csv')\n",
    "test_origin = pd.read_csv(data_path+'test_data.csv')\n",
    "topic_dict_origin = pd.read_csv(data_path+'topic_dict.csv')\n",
    "sample_submission_origin = pd.read_csv(data_path+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af7c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_origin.copy()\n",
    "test = test_origin.copy()\n",
    "topic_dict = topic_dict_origin.copy()\n",
    "sample_submission = sample_submission_origin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4360e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(\n",
    "    'kykim/bert-kor-base',\n",
    "    cache_dir = '../tokenizer/bert-base-multilingual-cased',\n",
    "    do_lower_case = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d990158",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {}\n",
    "data_config['max_length'] = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df6de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    text = re.sub('[^a-z가-힣]', ' ', text.lower())\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e352a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['title'].apply(cleaning_text)\n",
    "X_train = X_train.values.tolist()\n",
    "\n",
    "X_train_encoded = tokenizer(\n",
    "    X_train,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    max_length = data_config['max_length'],\n",
    "    return_tensors = 'tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8825312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['topic_idx'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d6dffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at kykim/bert-kor-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = transformers.TFBertModel.from_pretrained(\n",
    "    'kykim/bert-kor-base', \n",
    "    cache_dir = '../model/kykim/bert-kor-base/cache/',\n",
    "    output_hidden_states=False,\n",
    "    output_attentions=False,\n",
    "    use_cache = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08f370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(learning_rate=3e-5, print_summary=False):\n",
    "\n",
    "    input_ids = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='input_ids')\n",
    "    input_masks = Input(batch_shape=(None, data_config['max_length']), dtype=tf.int32, name='attention_masks')\n",
    "    bert_outputs = bert_model([input_ids, input_masks])['pooler_output']\n",
    "    dropout_0 = Dropout(0.1)(bert_outputs)\n",
    "    dense_0 = Dense(64, activation='relu', name='dense_0')(dropout_0)\n",
    "    dropout_1 = Dropout(0.1)(dense_0)\n",
    "    dense_1 = Dense(32, activation='relu', name='dense_1')(dropout_1)\n",
    "    dropout_2 = Dropout(0.1)(dense_1)\n",
    "    outputs = Dense(7, activation='softmax', name='outputs')(dropout_2)\n",
    "\n",
    "    model = Model(\n",
    "        inputs = [input_ids, input_masks],\n",
    "        outputs = outputs,\n",
    "        name = 'Bert_Classification'\n",
    "    )\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        optimizer = optimizer, \n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    if print_summary:\n",
    "        model.summary(line_length=150)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea7c744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f09d648bba8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f09d4339110> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f09d648bba8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f09d4339110> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Model: \"Bert_Classification\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 44)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 44)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 118297344   input_ids[0][0]                  \n",
      "                                                                 attention_masks[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 768)          0           tf_bert_model[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 64)           49216       dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 64)           0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2080        dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 32)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 7)            231         dropout_39[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 118,348,871\n",
      "Trainable params: 51,527\n",
      "Non-trainable params: 118,297,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = set_model(learning_rate=9e-5)\n",
    "model.layers[2].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17583885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "714/714 [==============================] - 226s 303ms/step - loss: 0.6608 - accuracy: 0.7995\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x = [X_train_encoded['input_ids'], X_train_encoded['attention_mask']], \n",
    "    y = y_train,\n",
    "    epochs = 1,\n",
    "    batch_size = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "909911c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    optimizer = optimizer, \n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6159145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1142/1142 [==============================] - ETA: 0s - loss: 0.3843 - accuracy: 0.8869WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1142/1142 [==============================] - 220s 185ms/step - loss: 0.3843 - accuracy: 0.8869 - val_loss: 0.3501 - val_accuracy: 0.8836\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.88358, saving model to ../model/bert_kor_base_fine_tuning_0.h5\n",
      "Epoch 2/15\n",
      "1142/1142 [==============================] - 209s 183ms/step - loss: 0.3006 - accuracy: 0.9118 - val_loss: 0.3629 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.88358\n",
      "Epoch 3/15\n",
      "1142/1142 [==============================] - 210s 184ms/step - loss: 0.2411 - accuracy: 0.9275 - val_loss: 0.4212 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.88358\n",
      "Epoch 4/15\n",
      "1142/1142 [==============================] - 212s 185ms/step - loss: 0.1799 - accuracy: 0.9475 - val_loss: 0.4151 - val_accuracy: 0.8864\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.88358 to 0.88643, saving model to ../model/bert_kor_base_fine_tuning_0.h5\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 5\n",
    ")\n",
    "model_check_point = ModelCheckpoint(\n",
    "    filepath = '../model/bert_kor_base_fine_tuning_0.h5',\n",
    "    monitor = 'val_accuracy',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True\n",
    ")\n",
    "def scheduler(epoch, lr):\n",
    "    return lr * 0.95\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "history = model.fit(\n",
    "    x = [X_train_encoded['input_ids'], X_train_encoded['attention_mask']], \n",
    "    y = y_train,\n",
    "    epochs = 15,\n",
    "    batch_size = 32,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [early_stop, model_check_point, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abab8c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "best_model = set_model()\n",
    "best_model.load_weights('../model/bert_kor_base_fine_tuning_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b66f1d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "train accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = best_model.predict([X_train_encoded['input_ids'], X_train_encoded['attention_mask']])\n",
    "y_train_pred = np.argmax(y_train_pred, axis=1)\n",
    "\n",
    "print(f'train accuracy: {accuracy_score(y_train, y_train_pred):.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2501c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test['title'].apply(cleaning_text)\n",
    "X_test = X_test.values.tolist()\n",
    "\n",
    "X_test_encoded = tokenizer(\n",
    "    X_test,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    max_length = data_config['max_length'],\n",
    "    return_tensors = 'tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b26f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict([X_test_encoded['input_ids'], X_test_encoded['attention_mask']])\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "sample_submission['topic_idx'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b4db88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission.to_csv('../submit/bert_kor_base_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ff66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
